{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Exercises: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope: \n",
    "Use Tensorflow 2.0 and pandas to build a simple linear regression model and then test the results on a hold-out set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Optimization\n",
    "\n",
    "To \"warm-up\", I'll use the tensorflow framework to solve a simple optimization problem. This will be done with the analytic gradient, and the autodifferentiation procedure which is standard in Tensorflow.\n",
    "\n",
    "First, define the function we will be using which is the Beale function:\n",
    "\n",
    "$$ f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2$$ \n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$  \\nabla f(x,y) =  \\begin{bmatrix}\n",
    "2 x (y^6 + y^4 - 2 y^3 - y^2 - 2 y + 3) + 5.25 y^3 + 4.5 y^2 + 3 y - 12.75 \\\\\n",
    " 6 x (x (y^5 + 0.666667 y^3 - y^2 - 0.333333 y - 0.333333) + 2.625 y^2 + 1.5 y + 0.5)\n",
    "\\end{bmatrix}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beale(x):\n",
    "    return (tf.convert_to_tensor( (1.5-x[0]+x[0]*x[1])**2 + (2.25-x[0]+x[0]*x[1]**2)**2 + (2.625-x[0]+x[0]*x[1]**3)**2) )\n",
    "\n",
    "def grad_beal(x):\n",
    "    dx = 2*x[0]*(x[1]**6 + x[1]**4 - 2*x[1]**3 - x[1]**2-2*x[1] + 3) + 5.25*x[1]**3+4.5*x[1]**2+3*x[1]-12.75\n",
    "    dy = 6*x[0]*(x[0]*(x[1]**5 + 2.0/3.0 * x[1]**3-x[1]**2-1.0/3.0 * x[1]-1.0/3.0) + 2.625*x[1]**2+1.5*x[1]+.5)\n",
    "    return(tf.convert_to_tensor([dx,dy]))\n",
    "\n",
    "def AD_beale(x):\n",
    "    x = tf.Variable(x,dtype = tf.float32) \n",
    "    with tf.GradientTape(persistent = True) as dv:\n",
    "        temp_beale = f_beale(x)\n",
    "    dx = dv.gradient(temp_beale, x)\n",
    "    return(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the results end up being the same for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_beal([1,1]) == AD_beale([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we will attemp to solve the problem using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2.9961617 , 0.49904037], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = .01)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = .01)\n",
    "x_0 = tf.Variable([1.0,1.0],dtype = tf.float32)\n",
    "func_for_opt = lambda: f_beale(x_0)\n",
    "\n",
    "for epoch in range(1500):\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "\n",
    "x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([156. 900. 132.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_vals = tf.Variable([1.0,2.0,3.0],name = \"x_vals\")\n",
    "x_vals_two = tf.Variable([2.0,3.0,4.0],name = \"x_vals_two\")\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x_vals[0]**3+x_vals[2]*x_vals[1]**2 + 2*x_vals[2]+100\n",
    "    z_one = x_vals[1]*(4*(y) + x_vals[0]**2)    \n",
    "    y_two = x_vals_two[0]**3+x_vals_two[2]*x_vals_two[1]**2 + 2*x_vals_two[2]+100\n",
    "    z_two = x_vals_two[1]*(4*(y_two) + x_vals_two[0]**2)\n",
    "    z = z_one + z_two\n",
    "    \n",
    "a, b = tape.gradient(z , [z,x_vals_two]) \n",
    "print(b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next solve a constrained optimization problem which is a constrained version of the Rosenbrock banana function:\n",
    "\n",
    "$$ \\min_{x,y} f(x,y)  =  (1-x)^2 + 100(y-x^2)$$\n",
    "\n",
    "Subject to:\n",
    "$$ (x-1)^3 - y + 1 <=0 $$\n",
    "\n",
    "and:\n",
    "\n",
    "$$ x+y-2<=0 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rosenbrock(vals):\n",
    "    return(tf.convert_to_tensor(  (1-vals[0])**2 + 100*(vals[1]-vals[0]**2)  ) )\n",
    "\n",
    "def const1_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*((vals[0]-vals[1])**3 - vals[1] + 1)   ))\n",
    "\n",
    "def const2_rosenbrock(vals):    \n",
    "    return(tf.convert_to_tensor( 1.0*(vals[0]+vals[1]-2)   ))\n",
    "\n",
    "def rosenbrock_lagrange(vals):\n",
    "    return(  f_rosenbrock(vals)+const1_rosenbrock(vals)+const2_rosenbrock(vals)   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-299.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rosenbrock([2.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-7,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 5e-6)\n",
    "x_0 = tf.Variable([.1,.1 ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(f_rosenbrock(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-4)&(i < 60000):\n",
    "#    print(i)\n",
    "    if i%1000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 5000 current results are [1.0583223 1.119438 ] with function value of 0.0006868541.\n",
      "At iteration 10000 current results are [1.0583223 1.1194379] with function value of 0.000674814.\n",
      "At iteration 15000 current results are [1.0583256 1.1194365] with function value of 0.00016772747.\n",
      "At iteration 20000 current results are [1.058328  1.1194353] with function value of 0.00078471005.\n",
      "At iteration 25000 current results are [1.0583247 1.1194369] with function value of 6.9633126e-05.\n",
      "At iteration 30000 current results are [1.0583297 1.1194346] with function value of 0.0012120008.\n",
      "At iteration 35000 current results are [1.0583258 1.1194364] with function value of 0.00020335615.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate = 1e-8,momentum = .9)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate = 1e-6)\n",
    "x_0 = tf.Variable([1.1,1.1  ],dtype = tf.float32)\n",
    "func_for_opt = lambda: tf.abs(rosenbrock_lagrange(x_0))\n",
    "\n",
    "\n",
    "#for epoch in range(1000):\n",
    "i=1\n",
    "while (func_for_opt().numpy()>1e-6)&(i < 100000):\n",
    "#    print(i)\n",
    "    if i%5000 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" current results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\")\n",
    "    opt.minimize(func_for_opt, [x_0]).numpy()\n",
    "    i+=1\n",
    "\n",
    "print(\"Final results at iteration \" + str(i) +  \" where the final results are \" + str(x_0.numpy() )+ \" with function value of \"+str( func_for_opt().numpy())  + \".\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data/Format Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show the basic steps of working through a mathematical optimization problem. At this point, we pivot to the use of the mathematical optimization framework to solve a statistical problem. \n",
    "\n",
    "Simple linear regression uses the mean squared error function to find a linear relationship between a set of features and an outcome. This is defined as:\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{\\sum_{i=1}^N (y_i - \\bar{\\beta}X_i )^2 }{N}$$\n",
    "\n",
    "Here, we step through a simple example of linear regression using tensorflow and an implementation of the MSE function.\n",
    "\n",
    "The functions are defined in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_val ,x_val,weights):\n",
    "    output = tf.tensordot(X, weights, axes=1 ) \n",
    "    mse_val =  tf.reduce_mean( tf.square( output - y_val ) )\n",
    "    return(mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this numerical example, we'll use the cars data-set and fit a few different variables to predict the gas mileage. \n",
    "\n",
    "First step: Load the data into the environment and form the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 100 function value is 67.86177.\n",
      "At iteration 200 function value is 43.66971.\n",
      "At iteration 300 function value is 30.864977.\n",
      "At iteration 400 function value is 24.923626.\n",
      "At iteration 500 function value is 22.539116.\n",
      "At iteration 600 function value is 21.708658.\n",
      "At iteration 700 function value is 21.443983.\n",
      "At iteration 800 function value is 21.349493.\n",
      "At iteration 900 function value is 21.296299.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( 'cars.csv' )\n",
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\",\"Engine Information.Engine Statistics.Torque\"] ].values / 100 \n",
    "X = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "X = np.append(np.ones((X.shape[0],1) ) , X, axis=1)\n",
    "Y = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "\n",
    "# Perform basic subset selection in the code:\n",
    "train_features , test_features ,train_labels, test_labels = train_test_split( X , Y , test_size=0.2 )\n",
    "# Training data.\n",
    "X = tf.Variable( train_features , dtype=tf.float32 )\n",
    "Y = tf.Variable( train_labels , dtype=tf.float32 )                                                         \n",
    "# Testing data\n",
    "test_X = tf.Variable( test_features , dtype=tf.float32 ) \n",
    "test_Y = tf.Variable( test_labels , dtype=tf.float32 ) \n",
    "num_features = X.shape[1]\n",
    "# Define the coefficientst that we'll be starting with:\n",
    "#weights = tf.Variable(tf.random.normal((num_features,1)))\n",
    "weights = tf.Variable(tf.ones(4))\n",
    "\n",
    "def MSE(y_val ,x_val,weights):\n",
    "    output = tf.tensordot(X, weights, axes=1 )\n",
    "    mse_val =  tf.reduce_mean( tf.square( output - y_val ) )\n",
    "    return(mse_val)\n",
    "\n",
    "#mse_opt = tf.keras.optimizers.SGD(.001)\n",
    "mse_opt = tf.keras.optimizers.Adam(.001)\n",
    "weight_vals = weights\n",
    "def temp_mse(weight_vals):\n",
    "    return(MSE(Y,X,weight_vals))\n",
    "func_for_opt = lambda: tf.abs(temp_mse(weight_vals))\n",
    "mse_opt.minimize(func_for_opt, [weight_vals]).numpy()\n",
    "\n",
    "i=1\n",
    "while (temp_mse(weight_vals).numpy()>1e-2)&(i < 1000):\n",
    "#    print(i)\n",
    "    if i%100 == 0:\n",
    "        print(\"At iteration \" + str(i) +  \" function value is \"+str( func_for_opt().numpy())  + \".\")\n",
    "    mse_opt.minimize(func_for_opt, [weight_vals]).numpy()    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "\n",
    "# Define lagrangian here by taking the sum of the two functions.\n",
    "def reg_lagrange( Y , y_pred_1, y_pred_2 ):\n",
    "    return tf.add(tf.reduce_mean( tf.square( y_pred_1 - Y ) ) ,  tf.reduce_mean( tf.square( y_pred_2 - Y ))) \n",
    "\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "#output = h( x_batch , weights , bias ) \n",
    "        output_1 = h( test_X_one , weights_one , bias_1 )\n",
    "        output_2 = h( test_X_one , weights_one , bias_2 )\n",
    "        loss = epoch_loss.append( reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    output_1\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            ret_val = reg_lagrange( Y=y_batch , y_pred_1 = output_1, y_pred_2 = output_2 )\n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = data[ [ \"Identification.Year\",\"Engine Information.Engine Statistics.Horsepower\"] ].values / 100 \n",
    "#categorical_research_features = data[ [ 'Research' ] ].values \n",
    "step1_vars = np.concatenate( [ continuous_features ] , axis=1 )\n",
    "step2_vars = np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )\n",
    "dep_var = data[ [ 'Fuel Information.City mpg' ] ].values\n",
    "test_X_one = tf.constant( step1_vars , dtype=tf.float32 ) \n",
    "\n",
    "num_epochs = 10\n",
    "num_samples = test_X_one.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "test_X_two = tf.constant( step2_vars , dtype=tf.float32 ) \n",
    "test_Y = tf.constant( dep_var , dtype=tf.float32 )\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( test_X_one , test_Y )) \n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "num_features = test_X_one.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    \n",
    "#    print( 'Loss is {}'.format( loss ) ) \n",
    "\n",
    "#Get predictions and then use them to create residuals.\n",
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1]     \n",
    "res_vals = preds-dep_var[:,0]        \n",
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = np.array(res_vals).tolist()\n",
    "test_X_two = tf.constant( np.concatenate( [ hold ], axis=1 ) , dtype=tf.float32 ) \n",
    " \n",
    "dataset_two = tf.data.Dataset.from_tensor_slices(( test_X_two , test_Y )) \n",
    "dataset_two = dataset_two.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset_two.__iter__()\n",
    "num_features = test_X_two.shape[1]\n",
    "weights = tf.random.normal( ( num_features , 1 ) )\n",
    "bias = 0\n",
    "        \n",
    "for i in range( num_epochs ) :    \n",
    "    epoch_loss = list()\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "   \n",
    "        output = h( x_batch , weights , bias ) \n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "    \n",
    "        output = tf.Variable( output, dtype=tf.float32 ) \n",
    "        y_batch = tf.Variable( y_batch, dtype=tf.float32 ) \n",
    "        with tf.GradientTape() as tape:\n",
    "            mse_val = tf.reshape( tf.reduce_mean( tf.square( output - y_batch ) )  , [ 1 , 1 ] )   \n",
    "    \n",
    "    \n",
    "        dJ_dH = tape.gradient(mse_val,output)\n",
    "        #dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "    \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB )      \n",
    "        \n",
    "        \n",
    "        \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    print( 'Loss is {}'.format( loss ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version below solves the problem as a lagrange optimization problem:\n",
    "\n",
    "\n",
    "$$ min(\\hat{y_2}  - y)^2 + \\lambda(\\hat{y_1}  - \\beta X)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reg_lagrange_problem( Y , y_one_pred, y_two_pred ):\n",
    "    return tf.reduce_mean( tf.square( y_pred - Y ) ) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] \n",
    "res_vals = preds-dep_var[:,0]\n",
    "res_vals = np.array(res_vals)\n",
    "#res_vals.resize([5076,0])\n",
    "res_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res_vals,step2_vars[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_vars[:,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate( [ data[ [\"Engine Information.Engine Statistics.Torque\"] ] ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = pd.DataFrame(step2_vars)\n",
    "hold[\"res\"] = res_vals.tolist()\n",
    "np.concatenate( [ hold ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((step2_vars, res_vals), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack(res_vals,step2_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg(indeps,deps,num_epochs = 10,batch_size = 50, learning_rate = .001):\n",
    "    num_samples = indeps.shape[0]\n",
    "    indeps = tf.constant( indeps , dtype=tf.float32 ) \n",
    "    deps = tf.constant( deps , dtype=tf.float32 ) \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(( indeps , deps )) \n",
    "    dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "    iterator = dataset.__iter__()\n",
    "    num_features = indeps.shape[1]\n",
    "    weights = tf.random.normal( ( num_features , 1 ) )\n",
    "    bias = 0\n",
    "    epochs_plot = list()\n",
    "    loss_plot = list()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = bias +  weights[0]*step1_vars[:,0]+  weights[1]*step1_vars[:,1] - dep_var\n",
    "pred_vals - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
