{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Work on Cora Data-set\n",
    "\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "To perform a simple testing exercise to try to predict the category of records contained in a data-set.This exercise will focus on simpler classification methods to predict results. This includes a graph embedding method (node2vec). See the original paper [here](https://arxiv.org/abs/1607.00653).\n",
    "\n",
    "\n",
    "\n",
    "## Data Description:\n",
    "\n",
    "**From the README:**\n",
    "\n",
    "The Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:\n",
    "\n",
    "\t\tCase_Based\n",
    "\t\tGenetic_Algorithms\n",
    "\t\tNeural_Networks\n",
    "\t\tProbabilistic_Methods\n",
    "\t\tReinforcement_Learning\n",
    "\t\tRule_Learning\n",
    "\t\tTheory\n",
    "\n",
    "The papers were selected in a way such that in the final corpus every paper cites or is cited by atleast one other paper. There are 2708 papers in the whole corpus. \n",
    "\n",
    "After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. All words with document frequency less than 10 were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install StellarGraph if running on Google Colab\n",
    "import sys\n",
    "import stellargraph as sg\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the frequency of connections is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2708, Edges: 5429\n",
      "\n",
      " Node types:\n",
      "  paper: [2708]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [5429]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "dataset = sg.datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "G, node_subjects = dataset.load()\n",
    "print(G.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node2vec is founded on using a random walk to sample the node choices over each of the nodes in the graph. Let $c_i$ be the $i$th node in the random walk where $c_i = u$ is the initial node. Then the random walk will be defined as:\n",
    "\n",
    "$$ P(c_i = x|c_{i-1}=v) = \\frac{\\pi_{vx}}{Z} \\text{ if }(v,x)\\in E$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\pi_{vx}$ is the unnormalized transition probability between node $x$ and $v$.\n",
    "* $Z$ is the normalizing constant.\n",
    "\n",
    "\n",
    "The code below implements the 2nd degree random walk on the graph. In this case, we are setting the maximum length of the random walk to 100, and are doing 10 walks per node. \n",
    "\n",
    "This is implemented and run through the BiasedRandomWalk algorithm below which will output a list of lists. Each list contains 100 components from the random walk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'walks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d66a58f7ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'walks' is not defined"
     ]
    }
   ],
   "source": [
    "len(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(G)\n",
    "walks = rw.run(\n",
    "    nodes=list(G.nodes()),  # root nodes\n",
    "    length=100,  # maximum length of a random walk\n",
    "    n=10,  # number of random walks per root node\n",
    "    p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the random walk algorithm produces a list of 100 node ids representing the \"neighborhood\". The first twenty ids from the first node are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks[1][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we generate the node embeddings for each neighborhood using the Word2vec algorithm. The list comprehension below is just there to format the ids into text strings. After that, the word2vec model is fit on the node lists to generate a list of node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=1)\n",
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = (model.wv.vectors)  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = node_subjects[[int(node_id) for node_id in node_ids]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of the embeddings is provided below;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str_walks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2)\n",
    "node_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "# draw the points\n",
    "alpha = 0.7\n",
    "label_map = {l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [label_map[target] for target in node_targets]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    node_embeddings_2d[:, 0],\n",
    "    node_embeddings_2d[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a vector of 100 features which represent the neighborhood of the model. This can be used with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 128-dimensional input features\n",
    "X = node_embeddings\n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.1, test_size=None)\n",
    "print(\"Array shapes:\\n X_train = {}\\n y_train = {}\\n X_test = {}\\n y_test = {}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "clf = LogisticRegressionCV(\n",
    "    Cs=10, cv=10, scoring=\"accuracy\", verbose=False, multi_class=\"ovr\", max_iter=300)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
